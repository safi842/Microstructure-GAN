{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport random\nfrom collections import OrderedDict\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.utils.data import DataLoader, Dataset\nimport pytorch_lightning as pl\nfrom torch.nn.utils import spectral_norm\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom torchvision.utils import make_grid\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#from torchsummaryX import summary","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ccbn(nn.Module):\n    def __init__(self, input_size, output_size, eps=1e-4, momentum=0.1):\n        super(ccbn, self).__init__()\n        self.output_size, self.input_size = output_size, input_size\n        # Prepare gain and bias layers\n        self.gain = spectral_norm(nn.Linear(input_size, output_size, bias = False), eps = 1e-4)\n        self.bias = spectral_norm(nn.Linear(input_size, output_size, bias = False), eps = 1e-4)\n        # epsilon to avoid dividing by 0\n        self.eps = eps\n        # Momentum\n        self.momentum = momentum\n        \n        self.register_buffer('stored_mean', torch.zeros(output_size))\n        self.register_buffer('stored_var',  torch.ones(output_size))\n    \n    def forward(self, x, y):\n        # Calculate class-conditional gains and biases\n        gain = (1 + self.gain(y)).view(y.size(0), -1, 1, 1)\n        bias = self.bias(y).view(y.size(0), -1, 1, 1)\n        out = F.batch_norm(x, self.stored_mean, self.stored_var, None, None,\n                          self.training, 0.1, self.eps)\n        return out * gain + bias\n    \n    def extra_repr(self):\n        s = 'out: {output_size}, in: {input_size},'\n        return s.format(**self.__dict__)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Self_Attn(nn.Module):\n    \"\"\" Self attention Layer\"\"\"\n    def __init__(self,in_dim,activation = nn.ReLU(inplace = False)):\n        super(Self_Attn,self).__init__()\n        self.chanel_in = in_dim\n        self.activation = activation\n        \n        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n        self.softmax  = nn.Softmax(dim=-1) #\n    def forward(self,x):\n        \"\"\"\n            inputs :\n                x : input feature maps( B X C X W X H)\n            returns :\n                out : self attention value + input feature \n                attention: B X N X N (N is Width*Height)\n        \"\"\"\n        m_batchsize,C,width ,height = x.size()\n        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n        energy =  torch.bmm(proj_query,proj_key) # transpose check\n        attention = self.softmax(energy) # BX (N) X (N) \n        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n\n        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n        out = out.view(m_batchsize,C,width,height)\n        \n        out = self.gamma*out + x\n        return out","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GeneratorResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, upsample = None, embed_dim = 128, dim_z = 128):\n        super(GeneratorResBlock, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hidden_channels = self.in_channels // 4\n        \n        self.conv1 = spectral_norm(nn.Conv2d(self.in_channels, self.hidden_channels, kernel_size = 1, padding = 0), eps = 1e-4)\n        self.conv2 = spectral_norm(nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size = 3, padding = 1), eps = 1e-4)\n        self.conv3 = spectral_norm(nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size = 3, padding = 1), eps = 1e-4)\n        self.conv4 = spectral_norm(nn.Conv2d(self.hidden_channels, self.out_channels, kernel_size = 1, padding = 0), eps = 1e-4)\n        \n        self.bn1 = ccbn(input_size = (3 * embed_dim) + dim_z, output_size = self.in_channels)\n        self.bn2 = ccbn(input_size = (3 * embed_dim) + dim_z, output_size = self.hidden_channels)\n        self.bn3 = ccbn(input_size = (3 * embed_dim) + dim_z, output_size = self.hidden_channels)\n        self.bn4 = ccbn(input_size = (3 * embed_dim) + dim_z, output_size = self.hidden_channels)\n        \n        self.activation = nn.ReLU(inplace=False)\n        \n        self.upsample = upsample\n        \n    def forward(self,x,y):\n        # Project down to channel ratio\n        h = self.conv1(self.activation(self.bn1(x, y)))\n        # Apply next BN-ReLU\n        h = self.activation(self.bn2(h, y))\n        # Drop channels in x if necessary\n        if self.in_channels != self.out_channels:\n            x = x[:, :self.out_channels]      \n        # Upsample both h and x at this point  \n        if self.upsample:\n            h = self.upsample(h)\n            x = self.upsample(x)\n        # 3x3 convs\n        h = self.conv2(h)\n        h = self.conv3(self.activation(self.bn3(h, y)))\n        # Final 1x1 conv\n        h = self.conv4(self.activation(self.bn4(h, y)))\n        return h + x","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, G_ch = 64, dim_z=128, bottom_width=4, img_channels = 1,\n                 init = 'ortho',n_classes_temp = 7, n_classes_time = 8, n_classes_cool = 4, embed_dim = 128):\n        super(Generator, self).__init__()\n        self.ch = G_ch\n        self.dim_z = dim_z\n        self.bottom_width = bottom_width\n        self.init = init\n        self.img_channels = img_channels\n\n        self.embed_temp = nn.Embedding(n_classes_temp, embed_dim)\n        self.embed_time = nn.Embedding(n_classes_time, embed_dim)\n        self.embed_cool = nn.Embedding(n_classes_cool, embed_dim)\n        \n        self.linear = spectral_norm(nn.Linear(dim_z + (3 * embed_dim), 16 * self.ch * (self.bottom_width **2)), eps = 1e-4)\n        \n        self.blocks = nn.ModuleList([\n                GeneratorResBlock(16*self.ch, 16*self.ch),\n                GeneratorResBlock(16*self.ch, 16*self.ch, upsample =  nn.Upsample(scale_factor = 2)),\n                GeneratorResBlock(16*self.ch, 16*self.ch),\n                GeneratorResBlock(16*self.ch, 8*self.ch, upsample =  nn.Upsample(scale_factor = 2)),\n                GeneratorResBlock(8*self.ch, 8*self.ch),\n                GeneratorResBlock(8*self.ch, 8*self.ch, upsample =  nn.Upsample(scale_factor = 2)),\n                GeneratorResBlock(8*self.ch, 8*self.ch),\n                GeneratorResBlock(8*self.ch, 4*self.ch, upsample =  nn.Upsample(scale_factor = 2)),\n                Self_Attn(4*self.ch),\n                GeneratorResBlock(4*self.ch, 4*self.ch),\n                GeneratorResBlock(4*self.ch, 2*self.ch, upsample =  nn.Upsample(scale_factor = 2)),\n                GeneratorResBlock(2*self.ch, 2*self.ch),\n                GeneratorResBlock(2*self.ch,  self.ch, upsample =  nn.Upsample(scale_factor = 2))\n        ])\n        \n        self.final_layer = nn.Sequential(\n                nn.BatchNorm2d(self.ch),\n                nn.ReLU(inplace = False),\n                spectral_norm(nn.Conv2d(self.ch, self.img_channels, kernel_size = 3, padding = 1)),\n                nn.Tanh()\n        )\n        \n        self.init_weights()\n                                    \n    def init_weights(self):\n        print(f\"Weight initialization : {self.init}\")\n        self.param_count = 0\n        for module in self.modules():\n            if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\n                if self.init == 'ortho':\n                    torch.nn.init.orthogonal_(module.weight)\n                elif self.init == 'N02':\n                    torch.nn.init.normal_(module.weight, 0, 0.02)\n                elif self.init in ['glorot', 'xavier']:\n                    torch.nn.init.xavier_uniform_(module.weight)\n                else:\n                    print('Init style not recognized...')\n                self.param_count += sum([p.data.nelement() for p in module.parameters()])\n        print(\"Param count for G's initialized parameters: %d Million\" % (self.param_count/1000000))\n        \n        \n    def forward(self,z , y_temp, y_time, y_cool):\n        y_temp = self.embed_temp(y_temp)\n        y_time = self.embed_time(y_time)\n        y_cool = self.embed_cool(y_cool)\n        z = torch.cat([z, y_temp, y_time, y_cool], 1)     \n        # First linear layer\n        h = self.linear(z)\n        # Reshape\n        h = h.view(h.size(0), -1, self.bottom_width, self.bottom_width)    \n        # Loop over blocks\n        for i, block in enumerate(self.blocks):\n            if i != 8:\n                h = block(h, z)\n            else:\n                h = block(h)\n        # Apply batchnorm-relu-conv-tanh at output\n        h = self.final_layer(h)\n        return h","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DiscriminatorResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, preactivation=True, \n                 downsample=None,channel_ratio=4):\n        super(DiscriminatorResBlock, self).__init__()\n        self.in_channels, self.out_channels = in_channels, out_channels\n        # If using wide D (as in SA-GAN and BigGAN), change the channel pattern\n        self.hidden_channels = self.out_channels // channel_ratio\n        self.preactivation = preactivation\n        self.activation = nn.ReLU(inplace=False)\n        self.downsample = downsample\n        \n        # Conv layers\n        self.conv1 = spectral_norm(nn.Conv2d(self.in_channels, self.hidden_channels, \n                                 kernel_size=1, padding=0), eps = 1e-4)\n        self.conv2 = spectral_norm(nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size = 3, padding = 1), eps = 1e-4)\n        self.conv3 = spectral_norm(nn.Conv2d(self.hidden_channels, self.hidden_channels, kernel_size = 3, padding = 1), eps = 1e-4)\n        self.conv4 = spectral_norm(nn.Conv2d(self.hidden_channels, self.out_channels, \n                                 kernel_size=1, padding=0), eps = 1e-4)\n                                 \n        self.learnable_sc = True if (in_channels != out_channels) else False\n        if self.learnable_sc:\n            self.conv_sc = spectral_norm(nn.Conv2d(in_channels, out_channels - in_channels, \n                                     kernel_size=1, padding=0), eps = 1e-4)\n            \n    def shortcut(self, x):\n        if self.downsample:\n            x = self.downsample(x)\n        if self.learnable_sc:\n            x = torch.cat([x, self.conv_sc(x)], 1)    \n        return x\n    \n    def forward(self, x):\n        # 1x1 bottleneck conv\n        h = self.conv1(F.relu(x))\n        # 3x3 convs\n        h = self.conv2(self.activation(h))\n        h = self.conv3(self.activation(h))\n        # relu before downsample\n        h = self.activation(h)\n        # downsample\n        if self.downsample:\n            h = self.downsample(h)     \n        # final 1x1 conv\n        h = self.conv4(h)\n        return h + self.shortcut(x)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, D_ch = 64, img_channels = 1, output_dim = 1,\n                 init = 'ortho',n_classes_temp = 7, n_classes_time = 8, n_classes_cool = 4):\n        super(Discriminator, self).__init__()\n        self.ch = D_ch\n        self.init = init\n        self.img_channels = img_channels\n        self.output_dim = output_dim\n        \n        # Prepare model\n        # Stem convolution\n        self.input_conv = spectral_norm(nn.Conv2d(self.img_channels, self.ch, kernel_size = 3, padding = 1), eps = 1e-4)\n        \n        self.blocks = nn.Sequential(\n                DiscriminatorResBlock(self.ch, 2*self.ch, downsample = nn.AvgPool2d(2)),\n                DiscriminatorResBlock(2*self.ch, 2*self.ch),\n                DiscriminatorResBlock(2*self.ch, 4*self.ch, downsample = nn.AvgPool2d(2)),\n                DiscriminatorResBlock(4*self.ch, 4*self.ch),\n                Self_Attn(4*self.ch),\n                DiscriminatorResBlock(4*self.ch, 8*self.ch, downsample = nn.AvgPool2d(2)),\n                DiscriminatorResBlock(8*self.ch, 8*self.ch),\n                DiscriminatorResBlock(8*self.ch, 8*self.ch, downsample = nn.AvgPool2d(2)),\n                DiscriminatorResBlock(8*self.ch, 8*self.ch),\n                DiscriminatorResBlock(8*self.ch, 16*self.ch, downsample = nn.AvgPool2d(2)),\n                DiscriminatorResBlock(16*self.ch, 16*self.ch),\n                DiscriminatorResBlock(16*self.ch, 16*self.ch, downsample = nn.AvgPool2d(2)),\n                DiscriminatorResBlock(16*self.ch, 16*self.ch),\n        )\n        # Linear output layer. The output dimension is typically 1, but may be\n        # larger if we're e.g. turning this into a VAE with an inference output\n        self.linear = spectral_norm(nn.Linear(16*self.ch, output_dim), eps = 1e-4)\n        # Embedding for projection discrimination\n        self.embed_temp = nn.Embedding(n_classes_temp, 16*self.ch)\n        self.embed_time = nn.Embedding(n_classes_time, 16*self.ch)\n        self.embed_cool = nn.Embedding(n_classes_cool, 16*self.ch)\n        \n        self.init_weights()\n    \n    def init_weights(self):\n        print(f\"Weight initialization : {self.init}\")\n        self.param_count = 0\n        for module in self.modules():\n            if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\n                if self.init == 'ortho':\n                    torch.nn.init.orthogonal_(module.weight)\n                elif self.init == 'N02':\n                    torch.nn.init.normal_(module.weight, 0, 0.02)\n                elif self.init in ['glorot', 'xavier']:\n                    torch.nn.init.xavier_uniform_(module.weight)\n                else:\n                    print('Init style not recognized...')\n                self.param_count += sum([p.data.nelement() for p in module.parameters()])\n        print(\"Param count for D's initialized parameters: %d Million\" % (self.param_count/1000000))\n        \n    def forward(self, x, y_temp, y_time, y_cool):\n        # Run input conv\n        h = self.input_conv(x)\n        # Blocks\n        h = self.blocks(h)\n        # Apply global sum pooling as in SN-GAN\n        h = torch.sum(nn.ReLU(inplace = False)(h), [2, 3])\n        # Get initial class-unconditional output\n        out = self.linear(h)\n        # Get projection of final featureset onto class vectors and add to evidence\n        out = out + torch.sum(self.embed_temp(y_temp) * h, 1, keepdim=True) + torch.sum(self.embed_time(y_time) * h, 1, keepdim=True) + torch.sum(self.embed_cool(y_cool) * h, 1, keepdim=True)\n        return out","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MicrographDataset(Dataset):\n    \"\"\"\n    A custom Dataset class for Micrograph data which returns the following\n    # Micrograph image\n    # Inputs : Anneal Temperature , Anneal Time and Type of cooling used\n    ------------------------------------------------------------------------------------\n    Attributes\n    \n    df : pandas.core.frame.DataFrame\n        A Dataframe that contains the proper entries (i.e. dataframe corresponding to new_metadata.xlsx)\n    root_dir : str\n        The path of the folder where the images are located\n    transform : torchvision.transforms.transforms.Compose\n        The transforms that are to be applied to the loaded images\n    \"\"\"\n    def __init__(self, df, root_dir, transform=None):\n        self.df = df\n        self.transform = transform\n        self.root_dir = root_dir\n        \n    def __len__(self):\n        return len(self.df)    \n    \n    def __getitem__(self, idx):\n        temp_dict = {970: 0, 800: 1, 900: 2, 1100: 3, 1000: 4, 700: 5, 750: 6}\n        time_dict = {90: 0, 1440: 1, 180: 2, 5: 3, 480: 4, 5100: 5, 60: 6, 2880: 7}\n        cooling_dict = {'Q': 0, 'FC': 1, 'AR': 2, '650-1H': 3}\n        row = self.df.loc[idx]\n        img_name = row['path']\n        img_path = self.root_dir + '/' + 'Cropped' + img_name\n        anneal_temp = temp_dict[row['anneal_temperature']]\n        if row['anneal_time_unit'] == 'H':\n            anneal_time = int(row['anneal_time']) * 60\n        else:\n            anneal_time = row['anneal_time']\n        anneal_time = time_dict[anneal_time]\n        cooling_type = cooling_dict[row['cool_method']]\n        img = Image.open(img_path)\n        img = img.convert('L')\n        if self.transform:\n            img = self.transform(img)\n        return img , anneal_temp, anneal_time, cooling_type","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MicrographBigGAN(pl.LightningModule):\n    def __init__(self, root_dir, df_dir, batch_size, lr = 2.5e-5):\n        super().__init__()\n        self.save_hyperparameters()\n        self.root_dir = root_dir\n        self.df_dir = df_dir\n        self.generator = Generator()\n        self.discriminator = Discriminator()\n        self.batch_size = batch_size \n        self.lr = lr\n        \n    def forward(self, z, y_temp, y_time, y_cool):\n        return self.generator(z, y_temp, y_time, y_cool)\n    \n    def discriminator_loss(self, disc_real, disc_fake):\n        loss_real = torch.mean(F.relu(1. - disc_real))\n        loss_fake = torch.mean(F.relu(1. + disc_fake))\n        return loss_real + loss_fake\n    \n    def generator_loss(self, disc_fake):\n        loss = -torch.mean(disc_fake)\n        return loss\n    \n    def training_step(self, batch, batch_idx, optimizer_idx):\n        real, y_temp, y_time, y_cool = batch\n        z = torch.randn(real.shape[0], 128)\n        z = z.type_as(real)\n        \n        if optimizer_idx == 0:\n            self.fake = self(z, y_temp, y_time, y_cool)\n            sample_imgs = real[:4]\n            grid = make_grid(sample_imgs)\n            self.logger.experiment.add_image('generated_images', grid, 0)\n        \n            disc_real = self.discriminator(real, y_temp, y_time, y_cool)\n            disc_fake = self.discriminator(self.fake, y_temp, y_time, y_cool)\n            d_loss = self.discriminator_loss(disc_real, disc_fake)\n            tqdm_dict = {'d_loss': d_loss}\n            output = OrderedDict({\n                'loss': d_loss,\n                'progress_bar': tqdm_dict,\n                'log': tqdm_dict\n            })\n            return output\n        \n        if optimizer_idx == 1:\n            self.fake = self(z,y_temp, y_time, y_cool)\n            g_loss = self.generator_loss(self.discriminator(self.fake, y_temp, y_time, y_cool))\n            tqdm_dict = {'g_loss': g_loss}\n            output = OrderedDict({\n                'loss': g_loss,\n                'progress_bar': tqdm_dict,\n                'log': tqdm_dict\n            })\n            return output\n        \n    def configure_optimizers(self):\n        opt_g = optim.Adam(self.generator.parameters(), lr = self.lr)\n        opt_d = optim.Adam(self.discriminator.parameters(), lr = self.lr)\n        return (\n            {'optimizer': opt_d, 'frequency': 2},\n            {'optimizer': opt_g, 'frequency': 1}\n        )\n    \n    def train_dataloader(self):\n        img_transforms = transforms.Compose([\n            transforms.Resize([256, 256]),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5 for _ in range(1)],[0.5 for _ in range(1)]),\n        ])\n        df = pd.read_excel(self.df_dir)\n        dataset = MicrographDataset(df,self.root_dir,transform = img_transforms)\n        return DataLoader(dataset, batch_size=self.batch_size,shuffle=True)\n    \n    def on_epoch_end(self):\n        # log sampled images\n        grid = make_grid(self.fake[:4])\n        self.logger.experiment.add_image('generated_images', grid, self.current_epoch)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT_DIR = '../input/highcarbon-micrographs/For Training/Cropped'\nDF_DIR = '../input/highcarbon-micrographs/new_metadata.xlsx'","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan = MicrographBigGAN(ROOT_DIR,DF_DIR,batch_size=8)","execution_count":11,"outputs":[{"output_type":"stream","text":"Weight initialization : ortho\nParam count for G's initialized parameters: 29 Million\nWeight initialization : ortho\nParam count for D's initialized parameters: 9 Million\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = pl.Trainer(max_epochs=100, gpus=1 if torch.cuda.is_available() else 0)","execution_count":12,"outputs":[{"output_type":"stream","text":"GPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from urllib.request import urlopen\nfrom io import BytesIO\nfrom zipfile import ZipFile\nfrom subprocess import Popen\nfrom os import chmod\nfrom os.path import isfile\nimport json\nimport time\nimport psutil\n\ndef launch_tensorboard():\n    tb_process, ngrok_process = None, None\n    \n    # Launch TensorBoard\n    if not is_process_running('tensorboard'):\n        tb_command = 'tensorboard --logdir ./lightning_logs --host 0.0.0.0 --port 6006'\n        tb_process = run_cmd_async_unsafe(tb_command)\n    \n    # Install ngrok\n    if not isfile('./ngrok'):\n        ngrok_url = 'https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip'\n        download_and_unzip(ngrok_url)\n        chmod('./ngrok', 0o755)\n\n    # Create ngrok tunnel and print its public URL\n    if not is_process_running('ngrok'):\n        ngrok_process = run_cmd_async_unsafe('./ngrok http 6006')\n        time.sleep(1) # Waiting for ngrok to start the tunnel\n    ngrok_api_res = urlopen('http://127.0.0.1:4040/api/tunnels', timeout=10)\n    ngrok_api_res = json.load(ngrok_api_res)\n    assert len(ngrok_api_res['tunnels']) > 0, 'ngrok tunnel not found'\n    tb_public_url = ngrok_api_res['tunnels'][0]['public_url']\n    print(f'TensorBoard URL: {tb_public_url}')\n\n    return tb_process, ngrok_process\n\n\ndef download_and_unzip(url, extract_to='.'):\n    http_response = urlopen(url)\n    zipfile = ZipFile(BytesIO(http_response.read()))\n    zipfile.extractall(path=extract_to)\n\n\ndef run_cmd_async_unsafe(cmd):\n    return Popen(cmd, shell=True)\n\n\ndef is_process_running(process_name):\n    running_process_names = (proc.name() for proc in psutil.process_iter())\n    return process_name in running_process_names\n\n\ntb_process, ngrok_process = launch_tensorboard()","execution_count":14,"outputs":[{"output_type":"stream","text":"TensorBoard URL: http://c184c32a5645.ngrok.io\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.fit(gan)","execution_count":13,"outputs":[{"output_type":"stream","text":"\n  | Name          | Type          | Params\n------------------------------------------------\n0 | generator     | Generator     | 29.5 M\n1 | discriminator | Discriminator | 9.1 M \n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce82d67467c7439394cd195e9ad881b6"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}